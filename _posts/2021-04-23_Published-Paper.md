---
title: Published Paper!
author: frosty
date: 2021-04-23 12:00:00 +0000
categories: [blog]
tags: [blog]     # TAG names should always be lowercase
---

Today I received positive news!

In 2020 I finished off my masters degree with my master thesis pursuing research in the IoT space regarding machine learning network intrusion detection systems. I really enjoyed this work, it gave me the opportunity to learn something brand new â€“ machine learning â€“ and couple it with a difficult challenge of creating an effective intrusion detection system.

Well, after I had my Viva VocÃ©, my supervisor suggested that we go ahead and try to publish this work. Today I received the notification that the paper has been accepted for publishing, after the reviews and corrections performed by some other staff members of the university.

You can read it on the Journal of Cybersecurity and Privacy on the MDPI website here: [Launching Adversarial Attacks against Network Intrusion Detection Systems for IoT](https://www.mdpi.com/2624-800X/1/2/14) ([DOI Link](https://doi.org/10.3390/jcp1020014)).

## Abstract of Paper:

As the internet continues to be populated with new devices and emerging technologies, the attack surface grows exponentially. Technology is shifting towards a profit-driven Internet of Things market where security is an afterthought. Traditional defending approaches are no longer sufficient to detect both known and unknown attacks to high accuracy. Machine learning intrusion detection systems have proven their success in identifying unknown attacks with high precision. Nevertheless, machine learning models are also vulnerable to attacks. Adversarial examples can be used to evaluate the robustness of a designed model before it is deployed. Further, using adversarial examples is critical to creating a robust model designed for an adversarial environment. Our work evaluates both traditional machine learning and deep learning modelsâ€™ robustness using the Bot-IoT dataset. Our methodology included two main approaches. First, label poisoning, used to cause incorrect classification by the model. Second, the fast gradient sign method, used to evade detection measures. The experiments demonstrated that an attacker could manipulate or circumvent detection with significant probability.

## Thanks!

Thanks again to my project supervisors (Dr. Nikolaos Pitropakis, and Dr. Christos Chrysoulas) as well as Pavlos Papadopoulos for your support in making this work ðŸ™‚

Thank you of course to the other collaborators of the project (Prof. William Buchanan, and Dr. Alexios Mylonas)
